{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f132059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d0115",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://github.com/PacktPublishing?page={}&tab=repositories\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "all_data = []\n",
    "\n",
    "for page in tqdm(range(1, 298)):  # —Å—Ç—Ä–∞–Ω–∏—Ü—ã 1-297\n",
    "    url = base_url.format(page)\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    repos = soup.select(\"li[itemprop='owns']\")\n",
    "\n",
    "    for repo in repos:\n",
    "        title_tag = repo.select_one(\"a[itemprop='name codeRepository']\")\n",
    "        if not title_tag:\n",
    "            continue\n",
    "        title = title_tag.text.strip()\n",
    "        href = \"https://github.com\" + title_tag['href']\n",
    "        stars_tag = repo.select_one(\"a[href$='/stargazers']\")\n",
    "        stars = stars_tag.text.strip() if stars_tag else \"0\"\n",
    "        all_data.append((title, href, stars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0664a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for title, _, _ in all_data:\n",
    "    words = re.findall(r'\\b\\w+\\b', title.lower())\n",
    "    all_words.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c044e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É\n",
    "url = \"https://github.com/PacktPublishing?page=1&tab=repositories\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# –ù–æ–≤—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä: –∏—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏\n",
    "repos = soup.select(\"a[href^='/PacktPublishing/']\")\n",
    "\n",
    "print(\"üîç –ù–∞–π–¥–µ–Ω–æ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤:\", len(repos))\n",
    "for a in repos[:5]:\n",
    "    title = a.text.strip()\n",
    "    href = \"https://github.com\" + a['href']\n",
    "    print(\"-\", title, \"‚Üí\", href)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c951eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "data = []\n",
    "\n",
    "for page in tqdm(range(1, 298)):  # –°—Ç—Ä–∞–Ω–∏—Ü—ã 1~297\n",
    "    url = f\"https://github.com/PacktPublishing?page={page}&tab=repositories\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2.5)  # –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –∏–Ω–∞—á–µ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–µ —É—Å–ø–µ—é—Ç –ø–æ—è–≤–∏—Ç—å—Å—è\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    repos = soup.select(\"a[href^='/PacktPublishing/']\")\n",
    "\n",
    "    for a in repos:\n",
    "        title = a.text.strip()\n",
    "        href = \"https://github.com\" + a['href']\n",
    "        if title:  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Å—ã–ª–∫–∏\n",
    "            data.append((title, href))\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# üìä –°–æ—Ö—Ä–∞–Ω–∏–º –≤ —Ç–∞–±–ª–∏—Ü—É\n",
    "df = pd.DataFrame(data, columns=[\"Title\", \"URL\"])\n",
    "df.to_excel(\"packt_repos.xlsx\", index=False)\n",
    "print(\"‚úÖ –ì–æ—Ç–æ–≤–æ! –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'packt_repos.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020077b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "all_words = []\n",
    "for title in df['Title']:\n",
    "    words = re.findall(r'\\b\\w+\\b', title.lower())  # —Ä–µ–≥—É–ª—è—Ä–∫–∞ = —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞\n",
    "    all_words.extend(words)\n",
    "\n",
    "top10 = Counter(all_words).most_common(10)\n",
    "print(\"üîù –¢–æ–ø-10 —Å–ª–æ–≤:\", top10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stopwords = set([\n",
    "    \"and\", \"with\", \"the\", \"for\", \"from\", \"using\", \"in\", \"of\", \"to\", \"on\", \"by\",\n",
    "    \"a\", \"an\", \"at\", \"as\", \"is\", \"this\", \"that\", \"you\", \"your\"\n",
    "])\n",
    "\n",
    "filtered = [w for w in all_words if w not in stopwords]\n",
    "text = \" \".join(filtered)\n",
    "\n",
    "wc = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c71973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "data = []\n",
    "\n",
    "for page in tqdm(range(1, 298)):  # 1~297\n",
    "    url = f\"https://github.com/PacktPublishing?page={page}&tab=repositories\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2.5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    repos = soup.select(\"li[itemprop='owns']\")\n",
    "\n",
    "    for repo in repos:\n",
    "        title_tag = repo.select_one(\"a[itemprop='name codeRepository']\")\n",
    "        if not title_tag:\n",
    "            continue\n",
    "        title = title_tag.text.strip()\n",
    "        href = \"https://github.com\" + title_tag['href']\n",
    "\n",
    "        star_tag = repo.select_one(\"a[href$='/stargazers']\")\n",
    "        stars = star_tag.text.strip() if star_tag else \"0\"\n",
    "\n",
    "        data.append((title, href, stars))\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel\n",
    "df = pd.DataFrame(data, columns=[\"Title\", \"URL\", \"Stars\"])\n",
    "df.to_excel(\"packt_books.xlsx\", index=False)\n",
    "print(\"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ packt_books.xlsx\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
